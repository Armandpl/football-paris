{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor 0 started\n",
      "actor 1 started\n",
      "actor 2 started\n",
      "actor 3 startedactor 4 started\n",
      "\n",
      "actor 5 started\n",
      "learner start\n",
      " data remaining. queue size :  389\n",
      " data remaining. queue size :  309\n",
      " data remaining. queue size :  229\n",
      " data remaining. queue size :  149\n",
      "10, Done, Step 188 Reward: 0.000000\n",
      "20, Done, Step 111 Reward: 1.000000\n",
      "30, Done, Step 136 Reward: 0.000000\n",
      "40, Done, Step 177 Reward: 0.000000\n",
      "50, Done, Step 65 Reward: 0.000000\n",
      "60, Done, Step 49 Reward: 1.000000\n",
      "70, Done, Step 43 Reward: 0.000000\n",
      "80, Done, Step 55 Reward: 0.000000\n",
      "90, Done, Step 113 Reward: -1.000000\n",
      "100, Done, Step 78 Reward: 1.000000\n",
      "110, Done, Step 63 Reward: 0.000000\n",
      "120, Done, Step 127 Reward: 0.000000\n",
      "130, Done, Step 44 Reward: 1.000000\n",
      "140, Done, Step 66 Reward: -1.000000\n",
      "150, Done, Step 44 Reward: 0.000000\n",
      "160, Done, Step 35 Reward: -1.000000\n",
      "170, Done, Step 54 Reward: 1.000000\n",
      "180, Done, Step 331 Reward: 0.000000\n",
      "190, Done, Step 34 Reward: 2.000000\n",
      "200, Done, Step 125 Reward: -2.000000\n",
      "210, Done, Step 401 Reward: 0.000000\n",
      "220, Done, Step 36 Reward: 0.000000\n",
      "230, Done, Step 39 Reward: 1.000000\n",
      "240, Done, Step 101 Reward: 0.000000\n",
      "250, Done, Step 160 Reward: -1.000000\n",
      "260, Done, Step 240 Reward: 0.000000\n",
      "270, Done, Step 91 Reward: -1.000000\n",
      "280, Done, Step 71 Reward: -1.000000\n",
      "290, Done, Step 126 Reward: 0.000000\n",
      "300, Done, Step 58 Reward: -2.000000\n",
      "310, Done, Step 38 Reward: 0.000000\n",
      "320, Done, Step 67 Reward: -1.000000\n",
      "330, Done, Step 45 Reward: 0.000000\n",
      "340, Done, Step 43 Reward: -1.000000\n",
      "350, Done, Step 51 Reward: 0.000000\n",
      "360, Done, Step 175 Reward: 0.000000\n",
      "370, Done, Step 140 Reward: 0.000000\n",
      "380, Done, Step 61 Reward: 0.000000\n",
      "390, Done, Step 61 Reward: -1.000000\n",
      "400, Done, Step 131 Reward: 0.000000\n",
      "410, Done, Step 76 Reward: 0.000000\n",
      "420, Done, Step 133 Reward: 0.000000\n",
      "430, Done, Step 124 Reward: 0.000000\n",
      "440, Done, Step 107 Reward: -1.000000\n",
      "450, Done, Step 60 Reward: 0.000000\n",
      "460, Done, Step 122 Reward: 0.000000\n",
      "470, Done, Step 63 Reward: 0.000000\n",
      "480, Done, Step 59 Reward: 1.000000\n",
      "490, Done, Step 386 Reward: -1.000000\n",
      "500, Done, Step 144 Reward: -1.000000\n",
      "510, Done, Step 133 Reward: 0.000000\n",
      "520, Done, Step 401 Reward: 1.000000\n",
      "530, Done, Step 50 Reward: -2.000000\n",
      "540, Done, Step 62 Reward: 0.000000\n",
      "550, Done, Step 39 Reward: 1.000000\n",
      "560, Done, Step 199 Reward: 0.000000\n",
      "570, Done, Step 53 Reward: 0.000000\n",
      "580, Done, Step 149 Reward: 0.000000\n",
      "590, Done, Step 33 Reward: 0.000000\n",
      "600, Done, Step 111 Reward: 1.000000\n",
      "610, Done, Step 43 Reward: -1.000000\n",
      "620, Done, Step 47 Reward: -1.000000\n",
      "630, Done, Step 34 Reward: -1.000000\n",
      "640, Done, Step 156 Reward: 0.000000\n",
      "650, Done, Step 57 Reward: 0.000000\n",
      "660, Done, Step 73 Reward: 1.000000\n",
      "670, Done, Step 53 Reward: 0.000000\n",
      "680, Done, Step 81 Reward: 1.000000\n",
      "690, Done, Step 47 Reward: 2.000000\n",
      "700, Done, Step 116 Reward: 1.000000\n",
      "710, Done, Step 39 Reward: 1.000000\n",
      "720, Done, Step 72 Reward: 0.000000\n",
      "730, Done, Step 66 Reward: 0.000000\n",
      "740, Done, Step 118 Reward: -3.000000\n",
      "750, Done, Step 146 Reward: -1.000000\n",
      "760, Done, Step 36 Reward: 0.000000\n",
      "770, Done, Step 401 Reward: 1.000000\n",
      "780, Done, Step 60 Reward: 1.000000\n",
      "790, Done, Step 152 Reward: -1.000000\n",
      "800, Done, Step 102 Reward: -1.000000\n",
      "810, Done, Step 107 Reward: 1.000000\n",
      "820, Done, Step 129 Reward: 0.000000\n",
      "830, Done, Step 121 Reward: -1.000000\n",
      "840, Done, Step 53 Reward: 1.000000\n",
      "850, Done, Step 117 Reward: 0.000000\n",
      "860, Done, Step 39 Reward: 2.000000\n",
      "870, Done, Step 210 Reward: 1.000000\n",
      "880, Done, Step 125 Reward: 1.000000\n",
      "890, Done, Step 41 Reward: 1.000000\n",
      "900, Done, Step 129 Reward: 0.000000\n",
      "910, Done, Step 126 Reward: -1.000000\n",
      "920, Done, Step 65 Reward: 0.000000\n",
      "930, Done, Step 128 Reward: 0.000000\n",
      "940, Done, Step 153 Reward: 1.000000\n",
      "950, Done, Step 124 Reward: 3.000000\n",
      "960, Done, Step 55 Reward: 0.000000\n",
      "970, Done, Step 53 Reward: 1.000000\n",
      "980, Done, Step 133 Reward: 1.000000\n",
      "990, Done, Step 128 Reward: 1.000000\n",
      "1000, Done, Step 64 Reward: 0.000000\n",
      "1010, Done, Step 123 Reward: 0.000000\n",
      "1020, Done, Step 118 Reward: -1.000000\n",
      "1030, Done, Step 84 Reward: 0.000000\n",
      "1040, Done, Step 60 Reward: -1.000000\n",
      "1050, Done, Step 61 Reward: 1.000000\n",
      "1060, Done, Step 103 Reward: 0.000000\n",
      "1070, Done, Step 131 Reward: 0.000000\n",
      "1080, Done, Step 68 Reward: 0.000000\n",
      "1090, Done, Step 122 Reward: 0.000000\n",
      "1100, Done, Step 65 Reward: 3.000000\n",
      "1110, Done, Step 72 Reward: -1.000000\n",
      "1120, Done, Step 78 Reward: 0.000000\n",
      "1130, Done, Step 129 Reward: 1.000000\n",
      "1140, Done, Step 116 Reward: 0.000000\n",
      "1150, Done, Step 149 Reward: 1.000000\n",
      "1160, Done, Step 341 Reward: 2.000000\n",
      "1170, Done, Step 102 Reward: 0.000000\n",
      "1180, Done, Step 76 Reward: 0.000000\n",
      "1190, Done, Step 74 Reward: 0.000000\n",
      "1200, Done, Step 224 Reward: 0.000000\n",
      "1210, Done, Step 79 Reward: -1.000000\n",
      "1220, Done, Step 298 Reward: 1.000000\n",
      "1230, Done, Step 94 Reward: 1.000000\n",
      "1240, Done, Step 71 Reward: 0.000000\n",
      "1250, Done, Step 67 Reward: -1.000000\n",
      "1260, Done, Step 96 Reward: 1.000000\n",
      "1270, Done, Step 214 Reward: 1.000000\n",
      "1280, Done, Step 81 Reward: 2.000000\n",
      "1290, Done, Step 73 Reward: -1.000000\n",
      "1300, Done, Step 62 Reward: 2.000000\n",
      "1310, Done, Step 88 Reward: 0.000000\n",
      "1320, Done, Step 80 Reward: 1.000000\n",
      "1330, Done, Step 76 Reward: 0.000000\n",
      "1340, Done, Step 85 Reward: 0.000000\n",
      "1350, Done, Step 75 Reward: 0.000000\n",
      "1360, Done, Step 75 Reward: 3.000000\n",
      "1370, Done, Step 401 Reward: 0.000000\n"
     ]
    }
   ],
   "source": [
    "import gfootball.env as football_env\n",
    "import time\n",
    "import pprint\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from FeatureEncoder import *\n",
    "from ppoLstm import *\n",
    "\n",
    "import torch.multiprocessing as mp \n",
    "import time\n",
    "        \n",
    "def actor(actor_num, center_model, data_queue, signal_queue, rollout_len):\n",
    "    print(\"actor {} started\".format(actor_num))\n",
    "    #11_vs_11_easy_stochastic\n",
    "    #academy_empty_goal_close 300 epi done\n",
    "    #academy_empty_goal 450 epi done\n",
    "    model = PPO()\n",
    "    env = football_env.create_environment(env_name=\"academy_empty_goal\", representation=\"raw\", stacked=False, logdir='/tmp/football', write_goal_dumps=False, write_full_episode_dumps=False, render=False)\n",
    "    fe = FeatureEncoder()\n",
    "    \n",
    "    n_epi = 0\n",
    "    score = 0\n",
    "    rollout = []\n",
    "    \n",
    "    while True:\n",
    "        env.reset()\n",
    "        done = False\n",
    "        steps = 0\n",
    "#         score = 0\n",
    "        n_epi += 1\n",
    "        h_out = (torch.zeros([1, 1, 256], dtype=torch.float), torch.zeros([1, 1, 256], dtype=torch.float))\n",
    "        \n",
    "        while not done:\n",
    "            t1 = time.time()\n",
    "            while signal_queue.qsize() > 0:\n",
    "                time.sleep(0.02)\n",
    "            else:\n",
    "                model.load_state_dict(center_model.state_dict())\n",
    "                \n",
    "            obs = env.observation()\n",
    "            state_dict = fe.encode(obs[0])\n",
    "            player_state = torch.from_numpy(state_dict[\"player\"]).float().unsqueeze(0).unsqueeze(0)\n",
    "            ball_state = torch.from_numpy(state_dict[\"ball\"]).float().unsqueeze(0).unsqueeze(0)\n",
    "            left_team_state = torch.from_numpy(state_dict[\"left_team\"]).float().unsqueeze(0).unsqueeze(0)\n",
    "            right_team_state = torch.from_numpy(state_dict[\"right_team\"]).float().unsqueeze(0).unsqueeze(0)\n",
    "            \n",
    "            h_in = h_out\n",
    "\n",
    "            state_dict_tensor = {\n",
    "              \"player\" : player_state,\n",
    "              \"ball\" : ball_state,\n",
    "              \"left_team\" : left_team_state,\n",
    "              \"right_team\" : right_team_state,\n",
    "              \"hidden\" : h_in\n",
    "            }\n",
    "\n",
    "            with torch.no_grad():\n",
    "                prob, _, h_out = model(state_dict_tensor)\n",
    "            m = Categorical(prob)\n",
    "            a = m.sample().item()\n",
    "\n",
    "            obs, rew, done, info = env.step(a)\n",
    "            state_prime_dict = fe.encode(obs[0])\n",
    "            \n",
    "            (h1_in, h2_in) = h_in\n",
    "            (h1_out, h2_out) = h_out\n",
    "            h_in_np = (h1_in.numpy(), h2_in.numpy())\n",
    "            h_out_np = (h1_out.numpy(), h2_out.numpy())\n",
    "            state_dict[\"hidden\"] = h_in_np\n",
    "            state_prime_dict[\"hidden\"] = h_out_np\n",
    "\n",
    "            transition = (state_dict, a, rew, state_prime_dict, prob[0][0][a].item(), done)\n",
    "            rollout.append(transition)\n",
    "\n",
    "            if len(rollout) == rollout_len:\n",
    "                data_queue.put(rollout)\n",
    "                rollout = []\n",
    "                \n",
    "            state_dict = state_prime_dict\n",
    "\n",
    "            steps += 1\n",
    "            score += rew\n",
    "\n",
    "            if done:\n",
    "                if n_epi % 10 == 0 and actor_num == 0:\n",
    "                    print(\"%d, Done, Step %d Reward: %f\" % (n_epi, steps, score))\n",
    "                    score = 0   \n",
    "\n",
    "\n",
    "def learner(center_model, queue, signal_queue, batch_size, buffer_size):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = PPO(device)\n",
    "    model.load_state_dict(center_model.state_dict())\n",
    "    model.to(device)\n",
    "    \n",
    "    print(\"learner start\")\n",
    "    \n",
    "    while True:\n",
    "        if queue.qsize() > batch_size*buffer_size:\n",
    "            signal_queue.put(1)\n",
    "            data = []\n",
    "            for j in range(buffer_size):\n",
    "                mini_batch_np = []\n",
    "                for i in range(batch_size):\n",
    "                    rollout = queue.get()\n",
    "                    mini_batch_np.append(rollout)\n",
    "                mini_batch = model.make_batch(mini_batch_np)\n",
    "                data.append(mini_batch)\n",
    "            model.train_net(data)\n",
    "            center_model.load_state_dict(model.state_dict())\n",
    "            \n",
    "            if queue.qsize() > batch_size*buffer_size:\n",
    "                print(\" data remaining. queue size : \", queue.qsize())\n",
    "            signal_queue.get()\n",
    "            \n",
    "        else:\n",
    "            time.sleep(0.1)\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # hyperparameters\n",
    "    num_processes = 6\n",
    "    batch_size = 16 # 16   # learner\n",
    "    buffer_size = 5 # 5    # learner\n",
    "    rollout_len = 10       # actor\n",
    "       \n",
    "    np.set_printoptions(precision=3)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    torch.set_num_threads(1)\n",
    "    \n",
    "    center_model = PPO()\n",
    "    center_model.share_memory()\n",
    "    data_queue = mp.Queue()\n",
    "    signal_queue = mp.Queue()\n",
    "    processes = []\n",
    "    \n",
    "    p = mp.Process(target=learner, args=(center_model, data_queue, signal_queue, batch_size, buffer_size))\n",
    "    p.start()\n",
    "    processes.append(p)\n",
    "    for rank in range(num_processes):\n",
    "        p = mp.Process(target=actor, args=(rank, center_model, data_queue, signal_queue, rollout_len))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "        \n",
    "    for p in processes:\n",
    "        p.join()\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-449866b2596c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0)\n",
    "x = x+1\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
